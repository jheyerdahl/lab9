---
title: "Lab9"
author: "Justin Heyerdahl"
date: "3/15/2018"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Step 1. Load packages.
```{r}
library(tidyverse)
library(cluster) #helps perform cluster analysis
library(NbClust) #helps decide how many clusters to break data into
library(factoextra) #visualize clusters
library(tidytext) #text based version of sf for spatial data
library(wordcloud) #makes a word cloud
```

##Step 2. Read in the files. 
```{r}
hate_crimes <- read_csv("hate_crimes.csv")
hawking_df <- read_csv("hawking_df.csv")
```

##Step 3. Basic data wrangling.
```{r}
#Perform listwise deletion of any rows with NA values.
crimes_df <- drop_na(hate_crimes)

#Use column_to_rownames and tell R which column you want to make as a the new row names.
crimes_df2 <- column_to_rownames(crimes_df, "state")

#scale() is generic function whose default method centers and/or scales the columns of a numeric matrix.
crimes_df2 <- scale(crimes_df2)
```

##Step 4. Calculate and Visualize Euclidean Distances
```{r}
#Use get_dist to get distances for your scaled data using the Euclidean method.
euc_dist <- get_dist(crimes_df2, method = "euclidean")

#Once you have Euclidean distances calculated, then you can use fviz_dist to visualize them.
fviz_dist(euc_dist, gradient = list(low = "darkgreen", mid = "white", high = "red"))
```

##Step 5. k-means Partition Based Cluster Analysis
```{r}
#Use NbClust to determine a estimate for the best number of clusters to use given the data.
crimes_no <- NbClust(crimes_df2, min.nc = 2, max.nc = 10, method = "kmeans")

#Run kmeans based on Euclidean distance (default), specifying the number of clusters, and iteratively figure out what the most logical clusters are). 
crimes_kmeans <- kmeans(crimes_df2, 3)

#Visualize the clusters from crimes_df, assigning clusters based on what we found in crimes_kmeans.
fviz_cluster(crimes_kmeans, data = crimes_df2)
#Large squares in the middle of the clusters indicate the centroid. This clustering is based on PCA, which is how these programs are determining out of all 12 variables how to plot the clusters in just 2-dimensions. Note the percentages on each of the axes, which indicate how much of the variance is explained in the first two principal components. In this case, around 70% total (axis 1 + axis 2.)

#You can experiment by changing the number of clusters you test for using kmeans(), and this will impact the number of clusters visualized with fviz_cluster(). Note that the amount of variance explained by the first to PCs doesn't change.

#If you find that when you re-run for the same number of clusters and it turns out differently, try re-running your whole script again. This should re-set it and bring back the same result again. Unlike with bootstrapping, the centroids and cluster outputs shouldn't change when you're specifying the same number of clusters again.
```

##Step 6. Hierarchical Clustering
```{r}
#Make a new distance dataframe to keep your next steps clean from the prior ones.
#It appears that get_dist() and dist() do the same thing (note we used get_dist() above).
d <- dist(crimes_df2, method = "euclidean")


#Agglomerative (bottom-up hierarchical clustering trees).
#Use method = complete so we start with individuals and go all the way to the root.
hc_crime <- agnes(d, method = "complete")
#Hang makes a hanging line so your text is even horizontally.

pltree(hc_crime, cex = 0.5, hang = -1)

#The dendrogram below is just a sequence of binary splits, which produces leafs. The first major split is between D.C. and all the rest, which recall was the most separate state in terms of differences across the 12 variables.
#Always look at the outcome and make sure it makes sense. For example, note that Washington and Oregon are closely matched, which makes sense since they are demographically similar states.
```

##Step 7. Get (rename hawking_df data). 
We will be doing text analysis on text-based data from Twitter. What we are hoping to do is pull out words that describe people's sentiments about Stephen Hawking's death.
```{r}
#Make a copy of the original so you don't overwrite it.
sh_df <- hawking_df

sh_text <- sh_df %>%
  #Select for only column "text"
  select(text) %>%
  #Unnest tokens, specify you want the unit "word" in the column "text". This separates out all the words from things like symbols, exclamation points, etc. This essentially removes any weird punctuation and pulls out the words themselves.
  unnest_tokens(word, text) %>%
  #Filter out some words that are super common and we aren't as interested in for this analysis.
  #Look for words that don't match the following strings.
  #Use the ! to tell tell R to retain the opposite of the command (i.e. remove the following words)
  filter(!word %in% c("hawking", "stephen", "died", "t.co", "https", "stephenhawking" ,"steven", "death"))

```

##Step 8. Getting counts of words that show up based on the 10,000 tweets.
```{r}

counts <- sh_text %>%
  #Remove any word that appears in that stop words collection.
  anti_join(stop_words) %>%
  #Remove anything with unique character strings in the word column
  count(word, sort = TRUE)

head(counts, 20)

```

##Step 9. Make a wordcloud.
```{r}
#Take the counts data we just created and with the wordcloud function, take everything that appears in the columns "word" and "n"
counts(wordcloud(word, n, max.words = 100, colors = brewer.pal(8, "Spectral")))
```


##Step 10. Sentiment Analysis
```{r}
#get_sentiments("afinn" %>% head(20))
#This is a score-based lexicon that ranks things based on very negative to very positive sentiments.

get_sentiments("nrc" %>% head(20))
#nrc lexicon assigns words to 6 or 7 categories of sentiments.
```

#Bind the sentiment words to the words that are in our dataset so R can find matches.
```{r}
sh_nrc <- sh_text %>% 
  #Get sentiments from the nrc lexicon and join them by words in the "word" column... 
  left_join(get_sentiments("nrc"), by = "word") %>%
  #and get rid of anything that has an NA value (meaning no match). Use the !
  filter(sentiment != "NA")
```

```{r}
#Sort by the top words and sentiments associatd with them.
counts_nrc <- sh_nrc %>%
  count(word, sentiment, sort = TRUE)

#Show me the first 20, aka the highest sentiments.
head(counts_nrc, 20)
```

```{r}
total_sentiment <- counts_nrc %>%
  #Group by sentiment.
  group_by(sentiment) %>%
  #Tally by totals in n column.
  summarise(totals = sum(n)) %>%
  #Arrange from high to low based on what's in the totals column.
  arrange(-totals)

total_sentiment
```

Plot these!
```{r}
ggplot(total_sentiment) +
  geom_col(aes(x = sentiment, y = totals))
```

